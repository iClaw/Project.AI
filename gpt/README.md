---
license: mit
language:
- en
- ru
tags:
- gpt3
- transformers
---
# üóø ruGPT-3.5 13B

Language model for Russian. Model has 13B parameters as you can guess from it's name. This is our biggest model so far and it was used for trainig GigaChat (read more about it in the [article](https://habr.com/ru/companies/sberbank/articles/730108/)).

## Dataset

Model was pretrained on a 300Gb of various domains, than additionaly trained on the 100 Gb of code and legal documets. Here is the dataset structure:

![](https://habrastorage.org/getpro/habr/upload_files/384/cd1/40f/384cd140fbd9b4e7dd5427801be13ca0.png)

Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.

## Technical details

Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data (see above).

After the final training perplexity for this model was around 8.8 for Russian.

![](https://i.imgur.com/0yx67yl.png)

## Examples of usage

Try different generation strategies to reach better results.

```python
request = "–°—Ç–∏—Ö –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–∞–∫–∏–º:"

encoded_input = tokenizer(request, return_tensors='pt', \
                          add_special_tokens=False).to('cuda:0')
output = model.generate(
    **encoded_input,
    num_beams=2,
    do_sample=True,
    max_new_tokens=100
)

print(tokenizer.decode(output[0], skip_special_tokens=True))
```

```
>>> –°—Ç–∏—Ö –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–∞–∫–∏–º:

    –ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç —Å–∏–¥–∏—Ç –≤ –∫—Ä–µ—Å–ª–µ,
    –°—Ç–∏—Ö —Å–æ—á–∏–Ω—è–µ—Ç –æ–Ω –ø—Ä–æ –ª—é–±–æ–≤—å,
    –û–Ω –ø–∏—à–µ—Ç, –ø–∏—à–µ—Ç, –ø–∏—à–µ—Ç, –ø–∏—à–µ—Ç...
    –ò –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç –Ω–∏ —á–µ—Ä—Ç–∞!
```

```python
request = "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å ‚Äî —ç—Ç–æ"

encoded_input = tokenizer(request, return_tensors='pt', \
                          add_special_tokens=False).to('cuda:0')
output = model.generate(
    **encoded_input,
    num_beams=4,
    do_sample=True,
    max_new_tokens=100
)

print(tokenizer.decode(output[0], skip_special_tokens=True))
```

```
>>> –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å ‚Äî —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å, —Å–æ—Å—Ç–æ—è—â–∞—è –∏–∑ –±–æ–ª—å—à–æ–≥–æ
    –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤, —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –º–µ–∂–¥—É —Å–æ–±–æ–π —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ —Å–≤—è–∑—è–º–∏.
    –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ, –∏ —Å –µ–µ –ø–æ–º–æ—â—å—é
    –º–æ–∂–Ω–æ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–¥–¥–∞—é—Ç—Å—è —Ä–µ—à–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö
    –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤.
```

```python
request = "–ì–∞–≥–∞—Ä–∏–Ω –ø–æ–ª–µ—Ç–µ–ª –≤ –∫–æ—Å–º–æ—Å –≤"

encoded_input = tokenizer(request, return_tensors='pt', \
                          add_special_tokens=False).to('cuda:0')
output = model.generate(
    **encoded_input,
    num_beams=2,
    do_sample=True,
    max_new_tokens=100
)

print(tokenizer.decode(output[0], skip_special_tokens=True))
```

```
>>> –ì–∞–≥–∞—Ä–∏–Ω –ø–æ–ª–µ—Ç–µ–ª –≤ –∫–æ—Å–º–æ—Å –≤ 1961 –≥–æ–¥—É. –≠—Ç–æ –±—ã–ª–æ –ø–µ—Ä–≤–æ–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏
    —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–µ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ. –Æ—Ä–∏–π –ì–∞–≥–∞—Ä–∏–Ω —Å–æ–≤–µ—Ä—à–∏–ª –µ–≥–æ
    –Ω–∞ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–º –∫–æ—Ä–∞–±–ª–µ –í–æ—Å—Ç–æ–∫-1. –ö–æ—Ä–∞–±–ª—å –±—ã–ª –∑–∞–ø—É—â–µ–Ω —Å –∫–æ—Å–º–æ–¥—Ä–æ–º–∞
    –ë–∞–π–∫–æ–Ω—É—Ä.
```